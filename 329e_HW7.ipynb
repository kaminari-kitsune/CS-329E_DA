{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7\n",
    "\n",
    "## Akshay Prakash\n",
    "\n",
    "## Supprt Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers here if you wish\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable inline mode for matplotlib so that Jupyter displays graphs\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You're allowed to use only the above libraries that are imported. No other libs should be used in this assignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heart Dataset \n",
    "\n",
    "In this Assignment we will work with some patients dataset. \n",
    "\n",
    "We have access to 303 patients data. The features are listed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>ChestPain</th>\n",
       "      <th>RestBP</th>\n",
       "      <th>Chol</th>\n",
       "      <th>Fbs</th>\n",
       "      <th>RestECG</th>\n",
       "      <th>MaxHR</th>\n",
       "      <th>ExAng</th>\n",
       "      <th>Oldpeak</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Thal</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>fixed</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>typical</td>\n",
       "      <td>110</td>\n",
       "      <td>264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>144</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>asymptomatic</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>reversable</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>nontypical</td>\n",
       "      <td>130</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>normal</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>nonanginal</td>\n",
       "      <td>138</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Sex     ChestPain  RestBP  Chol  Fbs  RestECG  MaxHR  ExAng  \\\n",
       "0     63    1       typical     145   233    1        2    150      0   \n",
       "1     67    1  asymptomatic     160   286    0        2    108      1   \n",
       "2     67    1  asymptomatic     120   229    0        2    129      1   \n",
       "3     37    1    nonanginal     130   250    0        0    187      0   \n",
       "4     41    0    nontypical     130   204    0        2    172      0   \n",
       "..   ...  ...           ...     ...   ...  ...      ...    ...    ...   \n",
       "298   45    1       typical     110   264    0        0    132      0   \n",
       "299   68    1  asymptomatic     144   193    1        0    141      0   \n",
       "300   57    1  asymptomatic     130   131    0        0    115      1   \n",
       "301   57    0    nontypical     130   236    0        2    174      0   \n",
       "302   38    1    nonanginal     138   175    0        0    173      0   \n",
       "\n",
       "     Oldpeak  Slope   Ca        Thal Target  \n",
       "0        2.3      3  0.0       fixed     No  \n",
       "1        1.5      2  3.0      normal    Yes  \n",
       "2        2.6      2  2.0  reversable    Yes  \n",
       "3        3.5      3  0.0      normal     No  \n",
       "4        1.4      1  0.0      normal     No  \n",
       "..       ...    ...  ...         ...    ...  \n",
       "298      1.2      2  0.0  reversable    Yes  \n",
       "299      3.4      2  2.0  reversable    Yes  \n",
       "300      1.2      2  1.0  reversable    Yes  \n",
       "301      0.0      2  1.0      normal    Yes  \n",
       "302      0.0      1  NaN      normal     No  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "heart_df = pd.read_csv(\"Heart.csv\")\n",
    "heart_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Age:** The personâ€™s age in years\n",
    "\n",
    "**Sex:** The personâ€™s sex (1 = male, 0 = female)\n",
    "\n",
    "**ChestPain:** chest pain type\n",
    "\n",
    "* Value 0: asymptomatic\n",
    "* Value 1: atypical angina\n",
    "* Value 2: non-anginal pain\n",
    "* Value 3: typical angina\n",
    "\n",
    "**RestBP:** The personâ€™s resting blood pressure (mm Hg on admission to the hospital)\n",
    "\n",
    "**Chol:** The personâ€™s cholesterol measurement in mg/dl\n",
    "\n",
    "**Fbs:** The personâ€™s fasting blood sugar (> 120 mg/dl, 1 = true; 0 = false)\n",
    "restecg: resting electrocardiographic results\n",
    "\n",
    "* Value 0: showing probable or definite left ventricular hypertrophy by Estesâ€™ criteria\n",
    "* Value 1: normal\n",
    "* Value 2: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "\n",
    "**RestECG:** The personâ€™s maximum heart rate achieved\n",
    "\n",
    "**MaxHR:** Exercise induced angina (1 = yes; 0 = no)\n",
    "\n",
    "**Oldpeak:** ST depression induced by exercise relative to rest (â€˜STâ€™ relates to positions on the ECG plot. See more here)\n",
    "\n",
    "**Slope:** the slope of the peak exercise ST segment â€” 0: downsloping; 1: flat; 2: upsloping\n",
    "\n",
    "* 0: downsloping; \n",
    "* 1: flat; \n",
    "* 2: upsloping\n",
    "\n",
    "**Ca:** The number of major vessels (0â€“3)\n",
    "\n",
    "**Thal:** A blood disorder called thalassemia Value 0: NULL (dropped from the dataset previously\n",
    "\n",
    "* Value 1: fixed defect (no blood flow in some part of the heart)\n",
    "* Value 2: normal blood flow\n",
    "* Value 3: reversible defect (a blood flow is observed but it is not normal)\n",
    "\n",
    "**Target:** Heart disease (1 = no, 0= yes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task - 1 (4 points)\n",
    "We want to use **Suppert Vector Machine** to perdict if the patients will have heart problems or not. The column \"Target\" in our datasets includes data about heart diseases. If the patient had heart disease we have a 1 and if not a zero. \n",
    "\n",
    "Prepare your data set for predicting heart disease (\"Target\" column) out of 3 features:\n",
    "\n",
    "* Age of the patient (Column **\"Age\"**)\n",
    "* Gender of the patient (male or female - Column **\"Sex\"**)\n",
    "* Cholestrol level of the patient (Column **\"Chol\"**) \n",
    "\n",
    "\n",
    "Split your data into 80% traning data and 20% test data, and implement Support Vector Machine using Scikit-Learn. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pre-processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Chol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Sex  Chol\n",
       "0     63    1   233\n",
       "1     67    1   286\n",
       "2     67    1   229\n",
       "3     37    1   250\n",
       "4     41    0   204\n",
       "..   ...  ...   ...\n",
       "298   45    1   264\n",
       "299   68    1   193\n",
       "300   57    1   131\n",
       "301   57    0   236\n",
       "302   38    1   175\n",
       "\n",
       "[303 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the x - dataframe - that containts the relevant independent variables / features being used to predict/ classify the condition.\n",
    "X = heart_df[[\"Age\", \"Sex\", \"Chol\"]]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create y - np array that\n",
    "y = np.where(heart_df[\"Target\"] == \"No\", 0, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 63,   1, 233],\n",
       "       [ 67,   1, 286],\n",
       "       [ 67,   1, 229],\n",
       "       [ 37,   1, 250],\n",
       "       [ 41,   0, 204],\n",
       "       [ 56,   1, 236],\n",
       "       [ 62,   0, 268],\n",
       "       [ 57,   0, 354],\n",
       "       [ 63,   1, 254],\n",
       "       [ 53,   1, 203],\n",
       "       [ 57,   1, 192],\n",
       "       [ 56,   0, 294],\n",
       "       [ 56,   1, 256],\n",
       "       [ 44,   1, 263],\n",
       "       [ 52,   1, 199],\n",
       "       [ 57,   1, 168],\n",
       "       [ 48,   1, 229],\n",
       "       [ 54,   1, 239],\n",
       "       [ 48,   0, 275],\n",
       "       [ 49,   1, 266],\n",
       "       [ 64,   1, 211],\n",
       "       [ 58,   0, 283],\n",
       "       [ 58,   1, 284],\n",
       "       [ 58,   1, 224],\n",
       "       [ 60,   1, 206],\n",
       "       [ 50,   0, 219],\n",
       "       [ 58,   0, 340],\n",
       "       [ 66,   0, 226],\n",
       "       [ 43,   1, 247],\n",
       "       [ 40,   1, 167],\n",
       "       [ 69,   0, 239],\n",
       "       [ 60,   1, 230],\n",
       "       [ 64,   1, 335],\n",
       "       [ 59,   1, 234],\n",
       "       [ 44,   1, 233],\n",
       "       [ 42,   1, 226],\n",
       "       [ 43,   1, 177],\n",
       "       [ 57,   1, 276],\n",
       "       [ 55,   1, 353],\n",
       "       [ 61,   1, 243],\n",
       "       [ 65,   0, 225],\n",
       "       [ 40,   1, 199],\n",
       "       [ 71,   0, 302],\n",
       "       [ 59,   1, 212],\n",
       "       [ 61,   0, 330],\n",
       "       [ 58,   1, 230],\n",
       "       [ 51,   1, 175],\n",
       "       [ 50,   1, 243],\n",
       "       [ 65,   0, 417],\n",
       "       [ 53,   1, 197],\n",
       "       [ 41,   0, 198],\n",
       "       [ 65,   1, 177],\n",
       "       [ 44,   1, 290],\n",
       "       [ 44,   1, 219],\n",
       "       [ 60,   1, 253],\n",
       "       [ 54,   1, 266],\n",
       "       [ 50,   1, 233],\n",
       "       [ 41,   1, 172],\n",
       "       [ 54,   1, 273],\n",
       "       [ 51,   1, 213],\n",
       "       [ 51,   0, 305],\n",
       "       [ 46,   0, 177],\n",
       "       [ 58,   1, 216],\n",
       "       [ 54,   0, 304],\n",
       "       [ 54,   1, 188],\n",
       "       [ 60,   1, 282],\n",
       "       [ 60,   1, 185],\n",
       "       [ 54,   1, 232],\n",
       "       [ 59,   1, 326],\n",
       "       [ 46,   1, 231],\n",
       "       [ 65,   0, 269],\n",
       "       [ 67,   1, 254],\n",
       "       [ 62,   1, 267],\n",
       "       [ 65,   1, 248],\n",
       "       [ 44,   1, 197],\n",
       "       [ 65,   0, 360],\n",
       "       [ 60,   1, 258],\n",
       "       [ 51,   0, 308],\n",
       "       [ 48,   1, 245],\n",
       "       [ 58,   1, 270],\n",
       "       [ 45,   1, 208],\n",
       "       [ 53,   0, 264],\n",
       "       [ 39,   1, 321],\n",
       "       [ 68,   1, 274],\n",
       "       [ 52,   1, 325],\n",
       "       [ 44,   1, 235],\n",
       "       [ 47,   1, 257],\n",
       "       [ 53,   0, 216],\n",
       "       [ 53,   0, 234],\n",
       "       [ 51,   0, 256],\n",
       "       [ 66,   1, 302],\n",
       "       [ 62,   0, 164],\n",
       "       [ 62,   1, 231],\n",
       "       [ 44,   0, 141],\n",
       "       [ 63,   0, 252],\n",
       "       [ 52,   1, 255],\n",
       "       [ 59,   1, 239],\n",
       "       [ 60,   0, 258],\n",
       "       [ 52,   1, 201],\n",
       "       [ 48,   1, 222],\n",
       "       [ 45,   1, 260],\n",
       "       [ 34,   1, 182],\n",
       "       [ 57,   0, 303],\n",
       "       [ 71,   0, 265],\n",
       "       [ 49,   1, 188],\n",
       "       [ 54,   1, 309],\n",
       "       [ 59,   1, 177],\n",
       "       [ 57,   1, 229],\n",
       "       [ 61,   1, 260],\n",
       "       [ 39,   1, 219],\n",
       "       [ 61,   0, 307],\n",
       "       [ 56,   1, 249],\n",
       "       [ 52,   1, 186],\n",
       "       [ 43,   0, 341],\n",
       "       [ 62,   0, 263],\n",
       "       [ 41,   1, 203],\n",
       "       [ 58,   1, 211],\n",
       "       [ 35,   0, 183],\n",
       "       [ 63,   1, 330],\n",
       "       [ 65,   1, 254],\n",
       "       [ 48,   1, 256],\n",
       "       [ 63,   0, 407],\n",
       "       [ 51,   1, 222],\n",
       "       [ 55,   1, 217],\n",
       "       [ 65,   1, 282],\n",
       "       [ 45,   0, 234],\n",
       "       [ 56,   0, 288],\n",
       "       [ 54,   1, 239],\n",
       "       [ 44,   1, 220],\n",
       "       [ 62,   0, 209],\n",
       "       [ 54,   1, 258],\n",
       "       [ 51,   1, 227],\n",
       "       [ 29,   1, 204],\n",
       "       [ 51,   1, 261],\n",
       "       [ 43,   0, 213],\n",
       "       [ 55,   0, 250],\n",
       "       [ 70,   1, 174],\n",
       "       [ 62,   1, 281],\n",
       "       [ 35,   1, 198],\n",
       "       [ 51,   1, 245],\n",
       "       [ 59,   1, 221],\n",
       "       [ 59,   1, 288],\n",
       "       [ 52,   1, 205],\n",
       "       [ 64,   1, 309],\n",
       "       [ 58,   1, 240],\n",
       "       [ 47,   1, 243],\n",
       "       [ 57,   1, 289],\n",
       "       [ 41,   1, 250],\n",
       "       [ 45,   1, 308],\n",
       "       [ 60,   0, 318],\n",
       "       [ 52,   1, 298],\n",
       "       [ 42,   0, 265],\n",
       "       [ 67,   0, 564],\n",
       "       [ 55,   1, 289],\n",
       "       [ 64,   1, 246],\n",
       "       [ 70,   1, 322],\n",
       "       [ 51,   1, 299],\n",
       "       [ 58,   1, 300],\n",
       "       [ 60,   1, 293],\n",
       "       [ 68,   1, 277],\n",
       "       [ 46,   1, 197],\n",
       "       [ 77,   1, 304],\n",
       "       [ 54,   0, 214],\n",
       "       [ 58,   0, 248],\n",
       "       [ 48,   1, 255],\n",
       "       [ 57,   1, 207],\n",
       "       [ 52,   1, 223],\n",
       "       [ 54,   0, 288],\n",
       "       [ 35,   1, 282],\n",
       "       [ 45,   0, 160],\n",
       "       [ 70,   1, 269],\n",
       "       [ 53,   1, 226],\n",
       "       [ 59,   0, 249],\n",
       "       [ 62,   0, 394],\n",
       "       [ 64,   1, 212],\n",
       "       [ 57,   1, 274],\n",
       "       [ 52,   1, 233],\n",
       "       [ 56,   1, 184],\n",
       "       [ 43,   1, 315],\n",
       "       [ 53,   1, 246],\n",
       "       [ 48,   1, 274],\n",
       "       [ 56,   0, 409],\n",
       "       [ 42,   1, 244],\n",
       "       [ 59,   1, 270],\n",
       "       [ 60,   0, 305],\n",
       "       [ 63,   0, 195],\n",
       "       [ 42,   1, 240],\n",
       "       [ 66,   1, 246],\n",
       "       [ 54,   1, 283],\n",
       "       [ 69,   1, 254],\n",
       "       [ 50,   1, 196],\n",
       "       [ 51,   1, 298],\n",
       "       [ 43,   1, 247],\n",
       "       [ 62,   0, 294],\n",
       "       [ 68,   0, 211],\n",
       "       [ 67,   1, 299],\n",
       "       [ 69,   1, 234],\n",
       "       [ 45,   0, 236],\n",
       "       [ 50,   0, 244],\n",
       "       [ 59,   1, 273],\n",
       "       [ 50,   0, 254],\n",
       "       [ 64,   0, 325],\n",
       "       [ 57,   1, 126],\n",
       "       [ 64,   0, 313],\n",
       "       [ 43,   1, 211],\n",
       "       [ 45,   1, 309],\n",
       "       [ 58,   1, 259],\n",
       "       [ 50,   1, 200],\n",
       "       [ 55,   1, 262],\n",
       "       [ 62,   0, 244],\n",
       "       [ 37,   0, 215],\n",
       "       [ 38,   1, 231],\n",
       "       [ 41,   1, 214],\n",
       "       [ 66,   0, 228],\n",
       "       [ 52,   1, 230],\n",
       "       [ 56,   1, 193],\n",
       "       [ 46,   0, 204],\n",
       "       [ 46,   0, 243],\n",
       "       [ 64,   0, 303],\n",
       "       [ 59,   1, 271],\n",
       "       [ 41,   0, 268],\n",
       "       [ 54,   0, 267],\n",
       "       [ 39,   0, 199],\n",
       "       [ 53,   1, 282],\n",
       "       [ 63,   0, 269],\n",
       "       [ 34,   0, 210],\n",
       "       [ 47,   1, 204],\n",
       "       [ 67,   0, 277],\n",
       "       [ 54,   1, 206],\n",
       "       [ 66,   1, 212],\n",
       "       [ 52,   0, 196],\n",
       "       [ 55,   0, 327],\n",
       "       [ 49,   1, 149],\n",
       "       [ 74,   0, 269],\n",
       "       [ 54,   0, 201],\n",
       "       [ 54,   1, 286],\n",
       "       [ 56,   1, 283],\n",
       "       [ 46,   1, 249],\n",
       "       [ 49,   0, 271],\n",
       "       [ 42,   1, 295],\n",
       "       [ 41,   1, 235],\n",
       "       [ 41,   0, 306],\n",
       "       [ 49,   0, 269],\n",
       "       [ 61,   1, 234],\n",
       "       [ 60,   0, 178],\n",
       "       [ 67,   1, 237],\n",
       "       [ 58,   1, 234],\n",
       "       [ 47,   1, 275],\n",
       "       [ 52,   1, 212],\n",
       "       [ 62,   1, 208],\n",
       "       [ 57,   1, 201],\n",
       "       [ 58,   1, 218],\n",
       "       [ 64,   1, 263],\n",
       "       [ 51,   0, 295],\n",
       "       [ 43,   1, 303],\n",
       "       [ 42,   0, 209],\n",
       "       [ 67,   0, 223],\n",
       "       [ 76,   0, 197],\n",
       "       [ 70,   1, 245],\n",
       "       [ 57,   1, 261],\n",
       "       [ 44,   0, 242],\n",
       "       [ 58,   0, 319],\n",
       "       [ 60,   0, 240],\n",
       "       [ 44,   1, 226],\n",
       "       [ 61,   1, 166],\n",
       "       [ 42,   1, 315],\n",
       "       [ 52,   1, 204],\n",
       "       [ 59,   1, 218],\n",
       "       [ 40,   1, 223],\n",
       "       [ 42,   1, 180],\n",
       "       [ 61,   1, 207],\n",
       "       [ 66,   1, 228],\n",
       "       [ 46,   1, 311],\n",
       "       [ 71,   0, 149],\n",
       "       [ 59,   1, 204],\n",
       "       [ 64,   1, 227],\n",
       "       [ 66,   0, 278],\n",
       "       [ 39,   0, 220],\n",
       "       [ 57,   1, 232],\n",
       "       [ 58,   0, 197],\n",
       "       [ 57,   1, 335],\n",
       "       [ 47,   1, 253],\n",
       "       [ 55,   0, 205],\n",
       "       [ 35,   1, 192],\n",
       "       [ 61,   1, 203],\n",
       "       [ 58,   1, 318],\n",
       "       [ 58,   0, 225],\n",
       "       [ 58,   1, 220],\n",
       "       [ 56,   1, 221],\n",
       "       [ 56,   1, 240],\n",
       "       [ 67,   1, 212],\n",
       "       [ 55,   0, 342],\n",
       "       [ 44,   1, 169],\n",
       "       [ 63,   1, 187],\n",
       "       [ 63,   0, 197],\n",
       "       [ 41,   1, 157],\n",
       "       [ 59,   1, 176],\n",
       "       [ 57,   0, 241],\n",
       "       [ 45,   1, 264],\n",
       "       [ 68,   1, 193],\n",
       "       [ 57,   1, 131],\n",
       "       [ 57,   0, 236],\n",
       "       [ 38,   1, 175]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the test/train split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementing the support-vector machine using the scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "model = svm.SVC()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - (4 points)\n",
    "\n",
    "Cacluate the accuracy, Precision, Recall and F1 score of your **SVM** implementaion. \n",
    "Print the results. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.80      0.67        35\n",
      "           1       0.46      0.23      0.31        26\n",
      "\n",
      "    accuracy                           0.56        61\n",
      "   macro avg       0.52      0.52      0.49        61\n",
      "weighted avg       0.53      0.56      0.52        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Implement SVM without using libraries  - (4 points)\n",
    "\n",
    "Implement SVM from scratch using Hinge Loss function and Gradient Descent. \n",
    "Try to produce the same result as you get from the libraries. \n",
    "\n",
    "\n",
    "* Do as many iterations as needed \n",
    "* Do maximum **100 iterations**\n",
    "* Use a very small learning rate for checking your GD implementation. \n",
    "* Your are allowed to use your choice of learning rate, like using 0.0001, 0.001 or 0.01 or 0.1 or higher. \n",
    "* Visualize your costs. \n",
    "* No need to add an y-intercept in this task. \n",
    "* You can use libraries to report accuracy, Precision, Recall and F1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, W, regularization_factor):\n",
    "    \n",
    "    n = X.shape[0]\n",
    "    distances = 1 - y * np.dot(X, W)\n",
    "    \n",
    "    distances[distances < 0] = 0\n",
    "    \n",
    "    hinge_loss = regularization_factor * (np.sum(distances) / n)\n",
    "    \n",
    "    return (1/2 * np.dot(W, W) + hinge_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient( X, y, W, regularization_factor):\n",
    "    \n",
    "    if type(y) == np.float64:\n",
    "        \n",
    "        y = np.array([y])\n",
    "        X = np.array([X])\n",
    "        \n",
    "    distance = 1 - ( y *  np.dot(X, W))\n",
    "    \n",
    "    dw = np.zeros(len(W))\n",
    "    \n",
    "    for ind, d in enumerate(distance):\n",
    "        \n",
    "        if (d < 0) :\n",
    "            di = W\n",
    "        else:\n",
    "            di = W - (regularization_factor * y[ind] * X[ind])\n",
    "            \n",
    "        dw += di\n",
    "        \n",
    "    dw = dw/len(y)\n",
    "    \n",
    "    return dw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Cost: 0.01 weights [0. 0. 0.]\n",
      "Epoch 1 Cost: 0.009851825212538558 weights [2.66115702e-05 3.71900826e-07 1.18785124e-04]\n",
      "Epoch 2 Cost: 0.009703680058552856 weights [5.32204793e-05 7.43764463e-07 2.37558369e-04]\n",
      "Epoch 3 Cost: 0.009555564532116503 weights [7.98267275e-05 1.11559091e-06 3.56319738e-04]\n",
      "Epoch 4 Cost: 0.009407478627304282 weights [1.06430315e-04 1.48738018e-06 4.75069230e-04]\n",
      "Epoch 5 Cost: 0.009259422338192163 weights [1.33031242e-04 1.85913227e-06 5.93806847e-04]\n",
      "Epoch 6 Cost: 0.009111395658857303 weights [1.59629509e-04 2.23084718e-06 7.12532590e-04]\n",
      "Epoch 7 Cost: 0.008963398583378043 weights [1.86225117e-04 2.60252492e-06 8.31246461e-04]\n",
      "Epoch 8 Cost: 0.008815431105833911 weights [2.12818064e-04 2.97416550e-06 9.49948460e-04]\n",
      "Epoch 9 Cost: 0.008667493220305612 weights [2.39408353e-04 3.34576891e-06 1.06863859e-03]\n",
      "Epoch 10 Cost: 0.008519584920875036 weights [2.65995982e-04 3.71733516e-06 1.18731685e-03]\n",
      "Epoch 11 Cost: 0.008371706201625268 weights [2.92580953e-04 4.08886425e-06 1.30598324e-03]\n",
      "Epoch 12 Cost: 0.008223857056640561 weights [3.19163265e-04 4.46035619e-06 1.42463777e-03]\n",
      "Epoch 13 Cost: 0.00807603748000636 weights [3.45742919e-04 4.83181098e-06 1.54328043e-03]\n",
      "Epoch 14 Cost: 0.007928247465809292 weights [3.72319915e-04 5.20322863e-06 1.66191122e-03]\n",
      "Epoch 15 Cost: 0.007780487008137159 weights [3.98894253e-04 5.57460913e-06 1.78053016e-03]\n",
      "Epoch 16 Cost: 0.007632756101078959 weights [4.25465934e-04 5.94595250e-06 1.89913723e-03]\n",
      "Epoch 17 Cost: 0.007485054738724862 weights [4.52034958e-04 6.31725873e-06 2.01773244e-03]\n",
      "Epoch 18 Cost: 0.00733738291516622 weights [4.78601325e-04 6.68852783e-06 2.13631579e-03]\n",
      "Epoch 19 Cost: 0.00718974062449557 weights [5.05165035e-04 7.05975980e-06 2.25488728e-03]\n",
      "Epoch 20 Cost: 0.007042149201691427 weights [5.31726088e-04 7.43095465e-06 2.37344692e-03]\n",
      "Epoch 21 Cost: 0.0068986666805411845 weights [5.58053081e-04 7.80211238e-06 2.49030461e-03]\n",
      "Epoch 22 Cost: 0.006755212854460346 weights [5.84377441e-04 8.17323300e-06 2.60715062e-03]\n",
      "Epoch 23 Cost: 0.006611787717710187 weights [6.10699169e-04 8.54431650e-06 2.72398495e-03]\n",
      "Epoch 24 Cost: 0.006469955314135959 weights [6.37018264e-04 8.91536290e-06 2.84080759e-03]\n",
      "Epoch 25 Cost: 0.006335742675498563 weights [6.63107455e-04 9.28223995e-06 2.95615988e-03]\n",
      "Epoch 26 Cost: 0.006220632625082862 weights [6.87532879e-04 9.63255140e-06 3.06189732e-03]\n",
      "Epoch 27 Cost: 0.006116261909755815 weights [7.11187267e-04 9.97456335e-06 3.16366138e-03]\n",
      "Epoch 28 Cost: 0.006029788635470839 weights [7.32996313e-04 1.02958799e-05 3.25522931e-03]\n",
      "Epoch 29 Cost: 0.005952806889166038 weights [7.53819708e-04 1.06047677e-05 3.34186659e-03]\n",
      "Epoch 30 Cost: 0.0058889335997834875 weights [7.73178210e-04 1.08929634e-05 3.42133406e-03]\n",
      "Epoch 31 Cost: 0.005834735737622711 weights [7.91125686e-04 1.11563369e-05 3.49377705e-03]\n",
      "Epoch 32 Cost: 0.005789837036883954 weights [8.07930871e-04 1.13990230e-05 3.56051445e-03]\n",
      "Epoch 33 Cost: 0.005751582771821678 weights [8.23279830e-04 1.16251558e-05 3.62054270e-03]\n",
      "Epoch 34 Cost: 0.005715339702425296 weights [8.38371055e-04 1.18512660e-05 3.67947816e-03]\n",
      "Epoch 35 Cost: 0.005684193387954152 weights [8.52783086e-04 1.20649569e-05 3.73517220e-03]\n",
      "Epoch 36 Cost: 0.005657914752391814 weights [8.65904419e-04 1.22620975e-05 3.78557141e-03]\n",
      "Epoch 37 Cost: 0.005633789286143544 weights [8.78578159e-04 1.24509540e-05 3.83386227e-03]\n",
      "Epoch 38 Cost: 0.005613633287690315 weights [8.90234103e-04 1.26273948e-05 3.87804913e-03]\n",
      "Epoch 39 Cost: 0.005594915999184864 weights [9.01434336e-04 1.27955536e-05 3.92018612e-03]\n",
      "Epoch 40 Cost: 0.005577667877366018 weights [9.12455763e-04 1.29595633e-05 3.96129824e-03]\n",
      "Epoch 41 Cost: 0.0055630893734514115 weights [9.22595922e-04 1.31111599e-05 3.99841450e-03]\n",
      "Epoch 42 Cost: 0.00554931610142999 weights [9.32511927e-04 1.32586091e-05 4.03453945e-03]\n",
      "Epoch 43 Cost: 0.005537023529091401 weights [9.41939337e-04 1.34019113e-05 4.06871865e-03]\n",
      "Epoch 44 Cost: 0.00552527898082131 weights [9.51126135e-04 1.35410670e-05 4.10192748e-03]\n",
      "Epoch 45 Cost: 0.0055146567590463385 weights [9.60035155e-04 1.36760765e-05 4.13418670e-03]\n",
      "Epoch 46 Cost: 0.005505599602768559 weights [9.68195349e-04 1.38028081e-05 4.16360386e-03]\n",
      "Epoch 47 Cost: 0.005497650065838799 weights [9.76164646e-04 1.39253948e-05 4.19206354e-03]\n",
      "Epoch 48 Cost: 0.005490867947698231 weights [9.83509178e-04 1.40397047e-05 4.21768979e-03]\n",
      "Epoch 49 Cost: 0.0054847475970461095 weights [9.90373637e-04 1.41498710e-05 4.24145810e-03]\n",
      "Epoch 50 Cost: 0.005478628470402914 weights [9.97237409e-04 1.42600262e-05 4.26522404e-03]\n",
      "Epoch 51 Cost: 0.005472510567523855 weights [1.00410050e-03 1.43701705e-05 4.28898760e-03]\n",
      "Epoch 52 Cost: 0.00546648463089305 weights [1.01096290e-03 1.44803037e-05 4.31274878e-03]\n",
      "Epoch 53 Cost: 0.005461209827044353 weights [1.01765932e-03 1.45862937e-05 4.33558610e-03]\n",
      "Epoch 54 Cost: 0.005456488572821642 weights [1.02387160e-03 1.46840086e-05 4.35661948e-03]\n",
      "Epoch 55 Cost: 0.005452456121470277 weights [1.02961632e-03 1.47734493e-05 4.37586151e-03]\n",
      "Epoch 56 Cost: 0.005448476367054351 weights [1.03536047e-03 1.48628811e-05 4.39510161e-03]\n",
      "Epoch 57 Cost: 0.005445234962613749 weights [1.04055445e-03 1.49440394e-05 4.41258772e-03]\n",
      "Epoch 58 Cost: 0.005442233737126381 weights [1.04548346e-03 1.50210574e-05 4.42919605e-03]\n",
      "Epoch 59 Cost: 0.0054392331118541 weights [1.05041197e-03 1.50980677e-05 4.44580271e-03]\n",
      "Epoch 60 Cost: 0.005436271239178872 weights [1.05533998e-03 1.51750703e-05 4.46240772e-03]\n",
      "Epoch 61 Cost: 0.005433575746650453 weights [1.06005263e-03 1.52479329e-05 4.47813503e-03]\n",
      "Epoch 62 Cost: 0.005430880793193583 weights [1.06476481e-03 1.53207883e-05 4.49386077e-03]\n",
      "Epoch 63 Cost: 0.005428186378700456 weights [1.06947651e-03 1.53936364e-05 4.50958494e-03]\n",
      "Epoch 64 Cost: 0.005425586700531089 weights [1.07418775e-03 1.54664772e-05 4.52530753e-03]\n",
      "Epoch 65 Cost: 0.005423178511313475 weights [1.07864644e-03 1.55351785e-05 4.54017319e-03]\n",
      "Epoch 66 Cost: 0.005421037688347213 weights [1.08285676e-03 1.55997407e-05 4.55418611e-03]\n",
      "Epoch 67 Cost: 0.00541889729352414 weights [1.08706666e-03 1.56642964e-05 4.56819763e-03]\n",
      "Epoch 68 Cost: 0.005416876311793102 weights [1.09127613e-03 1.57288457e-05 4.58220776e-03]\n",
      "Epoch 69 Cost: 0.005415080900085381 weights [1.09526205e-03 1.57892562e-05 4.59536524e-03]\n",
      "Epoch 70 Cost: 0.0054134245733442445 weights [1.09902029e-03 1.58496608e-05 4.60767430e-03]\n",
      "Epoch 71 Cost: 0.00541176857785189 weights [1.10277815e-03 1.59100593e-05 4.61998212e-03]\n",
      "Epoch 72 Cost: 0.005410217984746763 weights [1.10653565e-03 1.59704518e-05 4.63228872e-03]\n",
      "Epoch 73 Cost: 0.005408778686432991 weights [1.11007788e-03 1.60267060e-05 4.64375111e-03]\n",
      "Epoch 74 Cost: 0.005407339675964489 weights [1.11361977e-03 1.60829545e-05 4.65521236e-03]\n",
      "Epoch 75 Cost: 0.005405900953283692 weights [1.11716130e-03 1.61391975e-05 4.66667246e-03]\n",
      "Epoch 76 Cost: 0.005404462518333042 weights [1.12070248e-03 1.61954348e-05 4.67813141e-03]\n",
      "Epoch 77 Cost: 0.005403024371055 weights [1.12424330e-03 1.62516665e-05 4.68958921e-03]\n",
      "Epoch 78 Cost: 0.005401586511392031 weights [1.12778377e-03 1.63078926e-05 4.70104588e-03]\n",
      "Epoch 79 Cost: 0.005400148939286617 weights [1.13132388e-03 1.63641130e-05 4.71250139e-03]\n",
      "Epoch 80 Cost: 0.0053988631608288 weights [1.13486364e-03 1.64203278e-05 4.72395576e-03]\n",
      "Epoch 81 Cost: 0.005397805889836817 weights [1.13793611e-03 1.64724048e-05 4.73376849e-03]\n",
      "Epoch 82 Cost: 0.005396748830288461 weights [1.14100826e-03 1.65244766e-05 4.74358024e-03]\n",
      "Epoch 83 Cost: 0.005395691982141446 weights [1.14408011e-03 1.65765431e-05 4.75339100e-03]\n",
      "Epoch 84 Cost: 0.005394635345353489 weights [1.14715165e-03 1.66286045e-05 4.76320079e-03]\n",
      "Epoch 85 Cost: 0.0053935789198823235 weights [1.15022289e-03 1.66806606e-05 4.77300959e-03]\n",
      "Epoch 86 Cost: 0.005392585700444991 weights [1.15329382e-03 1.67327116e-05 4.78281741e-03]\n",
      "Epoch 87 Cost: 0.005391696222897457 weights [1.15608345e-03 1.67806251e-05 4.79182674e-03]\n",
      "Epoch 88 Cost: 0.005390806923236539 weights [1.15887280e-03 1.68285338e-05 4.80083516e-03]\n",
      "Epoch 89 Cost: 0.005389917801426659 weights [1.16166187e-03 1.68764377e-05 4.80984268e-03]\n",
      "Epoch 90 Cost: 0.00538905224852798 weights [1.16445066e-03 1.69243369e-05 4.81884929e-03]\n",
      "Epoch 91 Cost: 0.005388313283340655 weights [1.16705736e-03 1.69680990e-05 4.82704096e-03]\n",
      "Epoch 92 Cost: 0.005387574465938978 weights [1.16966379e-03 1.70118567e-05 4.83523181e-03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93 Cost: 0.005386837615693391 weights [1.17226997e-03 1.70556101e-05 4.84342184e-03]\n",
      "Epoch 94 Cost: 0.0053862337301452895 weights [1.17473125e-03 1.70952268e-05 4.85079287e-03]\n",
      "Epoch 95 Cost: 0.005385629965368259 weights [1.17719229e-03 1.71348396e-05 4.85816317e-03]\n",
      "Epoch 96 Cost: 0.0053850263213381456 weights [1.17965308e-03 1.71744484e-05 4.86553272e-03]\n",
      "Epoch 97 Cost: 0.005384422798030802 weights [1.18211363e-03 1.72140533e-05 4.87290154e-03]\n",
      "Epoch 98 Cost: 0.0053838193954220855 weights [1.18457393e-03 1.72536542e-05 4.88026962e-03]\n",
      "Epoch 99 Cost: 0.005383216113487856 weights [1.18703399e-03 1.72932512e-05 4.88763697e-03]\n"
     ]
    }
   ],
   "source": [
    "weights = np.zeros(3)\n",
    "\n",
    "num_iterations = 100\n",
    "learning_rate = 0.0001\n",
    "\n",
    "regularization = 0.01\n",
    "\n",
    "cost_list = []\n",
    "\n",
    "for i in range(0, num_iterations):\n",
    "    \n",
    "    cost = compute_cost(X_train, y_train, weights, regularization)\n",
    "    \n",
    "    print(\"Epoch\", i , \"Cost:\", cost, \"weights\", weights)\n",
    "    \n",
    "    cost_list.append(cost)\n",
    "    \n",
    "    grad = calculate_gradient(X_train, y_train, weights, regularization)\n",
    "    \n",
    "    weights = weights - learning_rate * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAlAUlEQVR4nO3de3xU9Z3/8ddnJjcggRAIyJ1gAopUESNivYJ2K/ZC3d6ktVrXllrBXvZqL9vHdn+73f112/21VbFaa9XaVVu1lbpWqyJeiijBWhEBCeEWrgEhEMh18vn9cQ4SY0IGSHIyM+/n4zGPzJzz/Z75fLmc93zPOZlj7o6IiGSeWNQFiIhINBQAIiIZSgEgIpKhFAAiIhlKASAikqEUACIiGUoBIJEws1VmdnEPv4ebWWn4/Kdm9s898B5/MLNrunu7Sbzvv5nZbjPb0dvvLenD9HsA0t3M7EngZXf/Trvlc4DbgdHu3tILdThQ5u6V3bS9fwFK3f2q7tjeCdQxBngLGOfuuzppMxD4V+CvgSJgB/AY8G/uvvs43/di4D53H308/aXv0QxAesLdwOfMzNot/xzwq97Y+ae5ccCeo+z8c4BngNOAy4CBwPuBPcD03ipSUoC766FHtz6AfkAtcGGbZYOBBuCM8PVG4NLw+XSgAtgP7AT+O1x+MVDdbtvt+70E7AO2A7cAOW3aOsEndghC6d/C578H6to8WoHPh+t+DGwJa1kBXBAuvwxoAprDPn8Jly8BvhA+jwHfBjYBu4B7gUHhuvFhPdcAm4HdwLeO8mc4KOxfE27v2+H2LwXqw5rrgLs76PuF8M8x/yjbPzWsfR+wCvhom3WXA28CB4CtwN8DA9q9bx0wMup/a3qc2EMzAOl27l4P/Bq4us3iTwFr3P0vHXT5MfBjdx8InBz2TUYC+DowFDgXuAS4IYn6PuLu+e6eD3yC4PDIM+Hq5cBUgsMm/wP8xszy3P0J4HvAg2HfMzrY9OfDx0xgApBPEEptnQ9MCmv9jpmd2kmZNxOEwATgIoI/y2vd/WlgNrAtrOPzHfS9FHjC3es62rCZZROE4B+BYcCNwK/MbFLY5OfAl9y9AJgCLHb3g+3eN9/dt3VSu6QIBYD0lHuAT5pZv/D11eGyjjQDpWY21N3r3H1ZMm/g7ivcfZm7t7j7RoLzCxclW6CZTST4lP1pd98SbvM+d98TbvOHQC7BDjsZnyWYvVSFO99vAFeaWVabNt919/owCP8CvCdIzCwOfBr4hrsfCMf2Q4JDaMkYQjAj6swMgnD6T3dvcvfFBOcH5obrm4HJZjbQ3fe6+6tJvq+kGAWA9Ah3f5Hg8MUcM5sAnE3wiboj1wETgTVmttzMPpzMe5jZRDN7zMx2mNl+gk/oQ5PsOwh4FPhnd3+hzfK/M7PVZlZrZvsIPoUntU1gJMHhmsM2AVnA8DbL2l61c4hgR9zeUCCng22NSrKOPcCILurc4u6tnWz/4wSHgTaZ2XNmdm6S7yspRgEgPelegk/+nwP+6O47O2rk7uvcfS7B4Yj/CzxkZgOAg0D/w+3CT8bFbbreBqwhuNJnIPBNoP2J5/cwsxhBGD3r7re3WX4B8E8Eh6sGu3shwbmMw9vs6pK5bQQnaA8bC7QQHI8/FrsJPoW339bWJPs/DXww/DPsrM4x4Z/De7bv7svdfQ7B38fvOHJITpcMphkFgPSkewmOR3+Rzg//YGZXmVlx+Il0X7g4QXCpY56ZfSg8bv1tgkMyhxUQnKytM7NTgC8nWde/E5zU/Gq75QUEO+waIMvMvkNwBc1hO4Hx7Xacbd0PfN3MSswsnyPnDI7pqid3TxDsdP/dzArMbBzwt8B9SW7ilwQnsh82s1PMLGZmQ8zsm2Z2OfAyQbj+o5llh5d3fgR4wMxyzOyzZjbI3ZsJ/nwTbcY/JJw9SRpQAEiPCY9dLyXY2S46StPLgFVmVkdwQvhKd29w91qCk7p3Enw6PQhUt+n398BnCK5W+RnwYJKlzSU4Dr7XzOrCx2eBJ4E/EATPJoKrlra06feb8OceM+vouPhdBDvf54ENYf8bk6ypvRsJxlsFvEgwY7krmY7u3kgQvGuApwh24q8QHFp62d2bgI8SnNTdDSwErnb3NeEmPgdsDA+rXQ9cFW53DUHIVZnZPjMbeZxjkz5CvwgmIpKhNAMQEclQCgARkQylABARyVAKABGRDJXVdZO+Y+jQoT5+/PioyxARSSkrVqzY7e7F7ZenVACMHz+eioqKqMsQEUkpZrapo+U6BCQikqEUACIiGUoBICKSoRQAIiIZSgEgIpKhkgoAM7vMzNaaWaWZ3dTBejOzn4TrXzezaW3W3WVmu8zsjXZ9iszsKTNbF/4cfOLDERGRZHUZAOF3sN9K8M2Bk4G5Zja5XbPZQFn4mEfwPe2H3U3wbY/t3QQ84+5lBLfje0+wiIhIz0lmBjAdqAxvc9cEPADMaddmDnCvB5YBhWY2AsDdnwfe7mC7czjyHfH3AB87jvqTsrRyNwuXVPbU5kVEUlIyATCKd38nejXvvTVdMm3aG+7u2wHCn8M6amRm88yswswqampqkij3vZ57q4YfPLmWjbsPHld/EZF0lEwAdHSLvfY3EUimzXFx9zvcvdzdy4uL3/ObzEm57oISsuIxbluyvjtKEhFJC8kEQDUwps3r0QT3FD3WNu3tPHyYKPy5K4lajsuwgjzmnj2Gh1+tZuu++p56GxGRlJJMACwHysL7nOYAV/Le2/stAq4OrwaaAdQePrxzFIuAa8Ln1wCPHkPdx2zeRSdjBrc/p1mAiAgkEQDhDa0XENwvdTXwa3dfZWbXm9n1YbPHCe5dWklwb9YbDvc3s/uBl4BJZlZtZteFq/4T+ICZrQM+EL7uMaMK+/HxaaN5YPkWdu1v6Mm3EhFJCSl1T+Dy8nI/kW8D3bTnIDN/sITrzi/hWx9qfyWriEh6MrMV7l7efnlG/SbwuCEDmDN1FPct28zbB5uiLkdEJFIZFQAAN1x8Mg0tCe56cUPUpYiIRCrjAqBseAGzp5zEPUs3UlvfHHU5IiKRybgAAJg/s5QDjS3cu3Rj1KWIiEQmIwPgtJGDuOSUYfz8Txs42NgSdTkiIpHIyAAAmD+rlH2HmvnVyx3eKlNEJO1lbABMGzuY80uHcsfzG2hoTkRdjohIr8vYAAC4cVYpu+saeeCVzVGXIiLS6zI6AM6ZMITp44u4/fkqGls0CxCRzJLRAQCwYFYp22sbeOTVrVGXIiLSqzI+AC4oG8oZowexcEklLYnWqMsREek1GR8AZsaCWWVsebueRX/p6husRUTSR8YHAMAlpwzjlJMKuOXZShKtqfPleCIiJ0IBAMRixoJZpVTVHOSJN3ZEXY6ISK9QAIRmTxnBhOIB3Lx4Ha2aBYhIBlAAhOIxY/7FpazZcYBn1vTY3SlFRPoMBUAbH506kjFF/bhl8TpS6UY5IiLHQwHQRnY8xpcvKuUv1bW8sG531OWIiPQoBUA7Hz9rFCMG5XHL4sqoSxER6VEKgHZys+J86cIJvLLxbZZV7Ym6HBGRHqMA6MCV08cyND+HW5/VLEBE0pcCoAN52XG+eMEEXli3mz9v3ht1OSIiPUIB0InPzhhHYf9snQsQkbSlAOhEfm4W151XwjNrdvHG1tqoyxER6XYKgKO4+v3jKcjNYuESzQJEJP0oAI5iUL9srnn/eP7wxg7W7TwQdTkiIt1KAdCFvzm/hH7ZcRYuWR91KSIi3UoB0IWiATlcNWMcj762lY27D0ZdjohIt1EAJOELF5SQFY9xm2YBIpJGFABJGFaQx9yzx/Dwq9Vs3VcfdTkiIt1CAZCkL110MmZw+3OaBYhIelAAJGlkYT8+cdZoHli+hV37G6IuR0TkhCkAjsGXLyol0er87IWqqEsRETlhCoBjMHZIf+acMZL7lm1mT11j1OWIiJyQpALAzC4zs7VmVmlmN3Ww3szsJ+H6181sWld9zewMM3vJzFaa2e/NbGD3DKln3TDzZBpaEtz1pw1RlyIickK6DAAziwO3ArOBycBcM5vcrtlsoCx8zANuS6LvncBN7v4+4LfAP5zwaHpB6bACLp8ygnuWbqL2UHPU5YiIHLdkZgDTgUp3r3L3JuABYE67NnOAez2wDCg0sxFd9J0EPB8+fwr4+AmOpdcsmFVKXWML97y0MepSRESOWzIBMArY0uZ1dbgsmTZH6/sG8NHw+SeBMR29uZnNM7MKM6uoqalJotyed+qIgVx66nDu+tMG6hpboi5HROS4JBMA1sEyT7LN0fr+DTDfzFYABUBTR2/u7ne4e7m7lxcXFydRbu+4cVYp+w41c9+yTVGXIiJyXJIJgGre/el8NLAtyTad9nX3Ne7+V+5+FnA/kFK/YXXGmEIuKBvKnS9UUd+UiLocEZFjlkwALAfKzKzEzHKAK4FF7dosAq4OrwaaAdS6+/aj9TWzYeHPGPBt4KfdMqJe9JVLythd18QDyzdHXYqIyDHrMgDcvQVYADwJrAZ+7e6rzOx6M7s+bPY4UAVUAj8Dbjha37DPXDN7C1hDMCv4RbeNqpecPb6Ic0qKuP25KhpbNAsQkdRi7u0P5/dd5eXlXlFREXUZ7/Liut1c9fOX+d4V7+Mz54yNuhwRkfcwsxXuXt5+uX4T+ASdVzqEqWMKWbikkuZEa9TliIgkTQFwgsyMG2eVUr23nkdfa39uXESk71IAdINZpwxj8oiBLHy2kkRr6hxSE5HMpgDoBodnAVW7D/L4yu1RlyMikhQFQDf54GknUTosn1sWV9KqWYCIpAAFQDeJxYwFM0tZu/MAT63eGXU5IiJdUgB0ow+fPoJxQ/pzy+JKUunyWhHJTAqAbpQVjzH/4lJWbq1lyVt944vrREQ6owDoZh87cxSjCvtpFiAifZ4CoJvlZMW4/qIJrNi0l5eq9kRdjohIpxQAPeCT5WMYVpDLLYsroy5FRKRTCoAekJcdZ96FE1i6fg8rNr0ddTkiIh1SAPSQz5wzlqIBOdysWYCI9FEKgB7SPyeL684vYcnaGlZW10ZdjojIeygAetDV545jYF4WNy9eF3UpIiLvoQDoQQV52Vx7Xgl/fHMna3bsj7ocEZF3UQD0sGvPG8+AnDi3PptStzwWkQygAOhhhf1z+Ny543ns9W2sr6mLuhwRkXcoAHrBFy4oITcrxkLNAkSkD1EA9IKh+bl8Zvo4fvfaVra8fSjqckREAAVAr5l34QTiZixcolmAiPQNCoBectKgPD519mgeXlHN9tr6qMsREVEA9KYvXXgyre7c/lxV1KWIiCgAetOYov5cceYo7n9lMzUHGqMuR0QynAKgl90ws5TmRCt3vqhZgIhESwHQy0qGDuAjZ4zkly9tYu/BpqjLEZEMpgCIwPyZpRxqSnDXnzZEXYqIZDAFQAQmDi9g9pSTuHvpRvY3NEddjohkKAVARObPLOVAQwv3Lt0YdSkikqEUABGZMmoQs04Zxs9f3MDBxpaoyxGRDKQAiNCCWaXsPdTM/7y8OepSRCQDKQAiNG3sYM4vHcrtz1fR0JyIuhwRyTAKgIgtmFXK7rpGHly+JepSRCTDKAAiNmPCEKaPL+Knz62nqaU16nJEJIMkFQBmdpmZrTWzSjO7qYP1ZmY/Cde/bmbTuuprZlPNbJmZvWZmFWY2vXuGlHoWzCple20DD79aHXUpIpJBugwAM4sDtwKzgcnAXDOb3K7ZbKAsfMwDbkui7/eB77r7VOA74euMdEHZUM4YPYiFSyppSWgWICK9I5kZwHSg0t2r3L0JeACY067NHOBeDywDCs1sRBd9HRgYPh8EbDvBsaQsM+PGWWVsebueR1/L2D8GEellyQTAKKDtGcrqcFkybY7W92vAf5nZFuAHwDc6enMzmxceIqqoqalJotzUdMmpwzh1xEBuXVJJotWjLkdEMkAyAWAdLGu/h+qszdH6fhn4uruPAb4O/LyjN3f3O9y93N3Li4uLkyg3NZkZC2aWUlVzkD+8sT3qckQkAyQTANXAmDavR/PewzWdtTla32uAR8LnvyE4XJTRZk85idJh+dyyuJJWzQJEpIclEwDLgTIzKzGzHOBKYFG7NouAq8OrgWYAte6+vYu+24CLwuezgHUnOJaUF4sZ82eezJodB3h69c6oyxGRNNdlALh7C7AAeBJYDfza3VeZ2fVmdn3Y7HGgCqgEfgbccLS+YZ8vAj80s78A3yO4eijjfeT0kYwb0p+bF1firlmAiPQcS6WdTHl5uVdUVERdRo97cPlm/unhldx97dlcPGlY1OWISIozsxXuXt5+uX4TuA+64szRjCrsp1mAiPQoBUAflJMV4/qLJrBi015eqtoTdTkikqYUAH3UJ8vHMKwgl5ufqYy6FBFJUwqAPiovO868CyfwUtUeVmx6O+pyRCQNKQD6sM+cM5aiATncvFizABHpfgqAPqx/ThZfuKCEJWtrWFldG3U5IpJmFAB93OdmjGNgXhY3L87435MTkW6mAOjjCvKyufa8Ev745k7W7NgfdTkikkYUACng2vPGk5+bxa3Pro+6FBFJIwqAFFDYP4fPnTuOx17fxvqauqjLEZE0oQBIEV84v4TcrBgLNQsQkW6iAEgRQ/Jz+ew54/jda1vZvOdQ1OWISBpQAKSQeRdOIB4zbntOswAROXEKgBQyfGAeny4fw0MrtrBtX33U5YhIilMApJjrLz4Zd7jj+aqoSxGRFKcASDGjCvvx8Wmjuf+Vzew60BB1OSKSwhQAKeiGmSfTnGjlzhc2RF2KiKQwBUAKGjdkAHOmjuK+ZZt4+2BT1OWISIpSAKSo+TNPpr45wV0vahYgIsdHAZCiSocVcPmUEdyzdCO19c1RlyMiKUgBkMLmzyzlQGML9yzdGHUpIpKCFAApbPLIgVx66nDu+tMG6hpboi5HRFKMAiDF3TirlH2Hmrlv2aaoSxGRFKMASHFnjCnkwonF3PlCFfVNiajLEZEUogBIAzfOKmV3XRP3v7I56lJEJIUoANLA2eOLmDGhiNufX09Ds2YBIpIcBUCauHFWGTv3N/LQiuqoSxGRFKEASBPvP3kI08YWctuS9TQnWqMuR0RSgAIgTZgZN84qY+u+en776taoyxGRFKAASCMXTypmyqiBLFxSSYtmASLSBQVAGjEzFswsY+OeQzz2+vaoyxGRPk4BkGb+avJwJg0v4JZnK2lt9ajLEZE+TAGQZmIxY/6sUip31fHEqh1RlyMifZgCIA196H0jmFA8gJsXV+KuWYCIdCypADCzy8xsrZlVmtlNHaw3M/tJuP51M5vWVV8ze9DMXgsfG83stW4ZkRCPGfMvLmX19v08vXpX1OWISB/VZQCYWRy4FZgNTAbmmtnkds1mA2XhYx5wW1d93f3T7j7V3acCDwOPdMeAJDBn6kjGDenPj595S7MAEelQMjOA6UClu1e5exPwADCnXZs5wL0eWAYUmtmIZPqamQGfAu4/wbFIG1nxGAtmlvLG1v08o1mAiHQgmQAYBWxp87o6XJZMm2T6XgDsdPd1Hb25mc0zswozq6ipqUmiXDnsijNHMW5If36kWYCIdCCZALAOlrXfm3TWJpm+cznKp393v8Pdy929vLi4+KiFyrtpFiAiR5NMAFQDY9q8Hg1sS7LNUfuaWRbw18CDyZcsx0KzABHpTDIBsBwoM7MSM8sBrgQWtWuzCLg6vBpoBlDr7tuT6HspsMbd9RWWPaTtLEBXBIlIW10GgLu3AAuAJ4HVwK/dfZWZXW9m14fNHgeqgErgZ8ANR+vbZvNXopO/Pe6dWcDTmgWIyBGWSjuE8vJyr6ioiLqMlPSbii38w0Ov87Ory/nA5OFRlyMivcjMVrh7efvl+k3gDKFZgIi0pwDIEIfPBazapnMBIhJQAGQQzQJEpC0FQAZpOwt46s2dUZcjIhFTAGSYI7OAdZoFiGQ4BUCGyYrHuHFWGW9u1yxAJNMpADLQx8JvCtUsQCSzKQAykGYBIgIKgIz1sakjGa9ZgEhGUwBkKM0CREQBkMHmaBYgktEUABksKx7jK5cEs4AnV2kWIJJpFAAZ7qNnjGRC8QB+9PRbtLZqFiCSSRQAGS4rHuOrl5SxZscBnli1I+pyRKQXKQCED58+ktJh+fy/p94ioVmASMZQAAjxmPHVS8pYt6uO/125PepyRKSXKAAEgA+9bwSThhfwo6feoiXRGnU5ItILFAACQCxm/P0HJ1G1+yC/rtAtmkUygQJA3nHpqcMoHzeYHz39FvVNiajLEZEepgCQd5gZ/zT7FHYdaOQXSzdEXY6I9DAFgLzL2eOLuOSUYdy2ZD37DjVFXY6I9CAFgLzHP1w2ibrGFhYuWR91KSLSgxQA8h6nnDSQT0wbzS/+tIF1Ow9EXY6I9BAFgHToptmnMCA3i2/+dqW+IkIkTSkApEND8nP55uWnsnzjXn6zYkvU5YhID1AASKc+edZoppcU8b3H17C7rjHqckSkmykApFNmxveumMKhphb+z2NvRl2OiHQzBYAcVemwAm64uJRHX9vGE2/o20JF0okCQLo0f2YpU0YN5Fu/XalDQSJpRAEgXcrJivHfn5rKgcYWvvHISt0+UiRNKAAkKROHF/CPH5zEU2/u5KEV+rI4kXSgAJCk/c15JZxTUsS/LFrFmh37oy5HRE6QAkCSFosZP5l7Jvl5WXzhngr26HyASEpLKgDM7DIzW2tmlWZ2Uwfrzcx+Eq5/3cymJdPXzG4M160ys++f+HCkpw0fmMfPri6n5kAj19+3gsYWfW20SKrqMgDMLA7cCswGJgNzzWxyu2azgbLwMQ+4rau+ZjYTmAOc7u6nAT/ojgFJzzt9dCE/+OQZLN+4l2//9g2dFBZJUcnMAKYDle5e5e5NwAMEO+625gD3emAZUGhmI7ro+2XgP929EcDdd3XDeKSXfOSMkXz1kjJ+s6Ka/3pybdTliMhxSCYARgFtvwymOlyWTJuj9Z0IXGBmL5vZc2Z29rEULtH72qVlfPacsSxcsp7bn9NXR4ukmqwk2lgHy9rP+Ttrc7S+WcBgYAZwNvBrM5vg7Y4nmNk8gsNKjB07NolypbeYGf86Zwr7G1r4jz+sYWC/bOZO19+RSKpIZgZQDYxp83o0sC3JNkfrWw08Eh42egVoBYa2f3N3v8Pdy929vLi4OIlypTfFY8YPP3kGF08q5pu/Xckv/qRbSYqkimQCYDlQZmYlZpYDXAksatdmEXB1eDXQDKDW3bd30fd3wCwAM5sI5AC7T3RA0vtysmL89Kqz+KvJw/nu79/kPx5frXsIiKSALg8BuXuLmS0AngTiwF3uvsrMrg/X/xR4HLgcqAQOAdcerW+46buAu8zsDaAJuKb94R9JHXnZcRZ+9iz+ZdEqbn++iu21DXz/E6eTlx2PujQR6YSl0j63vLzcKyoqoi5DjsLdue259Xz/ibWcOmIgt37mTCYU50ddlkhGM7MV7l7efrl+E1i6lZlxw8Wl/OLzZ7O9tp6P3Pwiv/9L+1NGItIXKACkR8w8ZRiPf+UCJp1UwI33/5n5//Mqu/Y3RF2WiLShAJAeM7KwHw9+6Vz+9gMTeerNnVzyw+f45UsbSegEsUifoACQHpUdj/GVS8p48msXcvqYQfzzo6uY/ePneerNnfoKCZGIKQCkV5QMHcB9153DrZ+ZRnPC+eK9FXzipy+xdL2u/BWJiq4Ckl7XnGjloRXV/PjpdezY38A5JUV8/QMTmTFhSNSliaSlzq4CUgBIZBqaEzzwymZuXbKemgONnDm2kGvPK2H2lJPIjmtyKtJdFADSZx0OgruXbmTjnkOcNDCPT5WP5oppoykZOiDq8kRSngJA+rzWVufZtbu4e+lGXqzcjTtMG1vInKmjmP2+kxhWkBd1iSIpSQEgKWV7bT2PvraNR16t5q2ddZjBOSVFfPC0k7hoYjElQwdg1tGXzYpIewoASVlv7TzA/76+nf9duZ3KXXUAjCnqxwVlxbz/5CGcO2EIQ/JzI65SpO9SAEha2LznEM+tq+G5tTUsq9pDXWMLAKecVMD0kiKmlxRRPq6IkwbpcJHIYQoASTstiVZWbq1l6fo9LKvaw4pNeznUFNykflhBLqePHsSUUYM4beQgThs5kBGD8nTYSDKSAkDSXnOilVXb9vPnzXtZubWWldW1VNbUcfifeGH/bCYOL2DS8AImDs+nbHgBZcPydfhI0l5nAZDMLSFFUkJ2PMbUMYVMHVP4zrJDTS2s3n6AN7fV8ub2/by1s47f/XkrB8JDRwCD+2czoTifkqEDKBk6gDFF/Rk9uB+jC/sxND+XWEyzBklPCgBJa/1zsjhr3GDOGjf4nWXuzvbaBip31bFuVx2Vuw6wYfdBXlhXw0Mrqt/VPztuDB+Yx4hBeQwbmEdxfi7FBbkU5+dSNCCHIfk5DM3PZUh+Dv1z9N9JUov+xUrGMTNGFvZjZGE/Lpz47vtMH2pqoXpvPdV7D7Hl7Xq21zawo7aebbUNrN62n+cPNL5r9tBWv+w4Q/JzGJKfy9ABORQNyKEoP4ehA3IZPCCHwf2zKeyfQ2H/bAb1Cx76jWeJkgJApI3+OVlMHF7AxOEFnbapb0qwu66Rtw82sbuukT0Hm9hT18Sew8sONrG9toFV2/bz9sEmmhKtnW6rX3acgrws8vOyKMjNoiAvm/zc4HX/nDj9cuIMyAme52XH6Z8Tb/P83cv7ZQfPc7NiOmwlSVEAiByjfjlxxhT1Z0xR/y7bujsHGlvYe7CJvYea2XeoiX2Hmtnf0Ext+LOusYUDDcGjrrGFmgONHGho5lBzgkNNCZpaOg+QzuRmxeiXEycvKwiRvOw4/bJj5IUhkZcdIy8rTm4YGHnZcXKyYuS+63FkWU74yI4Hj9x3nhvZ8SPrsuJGTtgmrhDq8xQAIj3IzBiYl83AvGzGHeeXnTYnWqlvTlDfFARCfVPindf1zQkONbVQ35SgoTlBfXPQtrE5eH2oKUFDS+s76xuaE+xvaKahuZWG5gSNLUd+Hk/QHH3skB0LQiErZu8ERFbsSHBkhSGSFTOy4jFy2rXJisfIjlmw7J3nR7aZFYuRFTPicSNuRjwWLI/HjPjhdW0eh19nxY2YBf1jMciKxYjHeNeymLXpGz6Phds4vC5mEAvXx8yO9DNLiVmYAkCkjzv8qXtgXnaPvo+705RopaE5CIOmRCuNzQmaEsHrxpZWmsPlTS2ttLQ6zeHz5kSb562ttISvmxNOS+JI25aEv7O+pbWVppbg5+H29c0JWhrCfuHypsSR9m2319Lqff7ucjGDeMwwC8MiDAoz3hUiR1t/+Pn3rngf00uKurU+BYCIAMFOKDcrTm5WPOpSkuYehMDhMEi4k0gEr1s9+NmSaCURvm5OhO1a27RJHOnb2mZb7uH2Wo88Wt1JtEKitZVW551lh5e3httIuNPqwRccth5+7sE2D/d757k77kfG4hzZzpF+MCC3+/9eFAAikrLMDh8eirqS1KRr0EREMpQCQEQkQykAREQylAJARCRDKQBERDKUAkBEJEMpAEREMpQCQEQkQ6XUHcHMrAbYdJzdhwK7u7GcVJGJ487EMUNmjjsTxwzHPu5x7l7cfmFKBcCJMLOKjm6Jlu4ycdyZOGbIzHFn4pih+8atQ0AiIhlKASAikqEyKQDuiLqAiGTiuDNxzJCZ487EMUM3jTtjzgGIiMi7ZdIMQERE2lAAiIhkqIwIADO7zMzWmlmlmd0UdT09wczGmNmzZrbazFaZ2VfD5UVm9pSZrQt/Do661u5mZnEz+7OZPRa+zoQxF5rZQ2a2Jvw7Pzfdx21mXw//bb9hZvebWV46jtnM7jKzXWb2RptlnY7TzL4R7tvWmtkHj+W90j4AzCwO3ArMBiYDc81scrRV9YgW4O/c/VRgBjA/HOdNwDPuXgY8E75ON18FVrd5nQlj/jHwhLufApxBMP60HbeZjQK+ApS7+xQgDlxJeo75buCydss6HGf4f/xK4LSwz8Jwn5eUtA8AYDpQ6e5V7t4EPADMibimbufu29391fD5AYIdwiiCsd4TNrsH+FgkBfYQMxsNfAi4s83idB/zQOBC4OcA7t7k7vtI83ET3MK2n5llAf2BbaThmN39eeDtdos7G+cc4AF3b3T3DUAlwT4vKZkQAKOALW1eV4fL0paZjQfOBF4Ghrv7dghCAhgWYWk94UfAPwKtbZal+5gnADXAL8JDX3ea2QDSeNzuvhX4AbAZ2A7UuvsfSeMxt9PZOE9o/5YJAWAdLEvba1/NLB94GPiau++Pup6eZGYfBna5+4qoa+llWcA04DZ3PxM4SHoc+uhUeMx7DlACjAQGmNlV0VbVJ5zQ/i0TAqAaGNPm9WiCqWPaMbNsgp3/r9z9kXDxTjMbEa4fAeyKqr4ecB7wUTPbSHBob5aZ3Ud6jxmCf9PV7v5y+PohgkBI53FfCmxw9xp3bwYeAd5Peo+5rc7GeUL7t0wIgOVAmZmVmFkOwQmTRRHX1O3MzAiOCa929/9us2oRcE34/Brg0d6urae4+zfcfbS7jyf4e13s7leRxmMGcPcdwBYzmxQuugR4k/Qe92Zghpn1D/+tX0Jwniudx9xWZ+NcBFxpZrlmVgKUAa8kvVV3T/sHcDnwFrAe+FbU9fTQGM8nmPq9DrwWPi4HhhBcNbAu/FkUda09NP6LgcfC52k/ZmAqUBH+ff8OGJzu4wa+C6wB3gB+CeSm45iB+wnOczQTfMK/7mjjBL4V7tvWArOP5b30VRAiIhkqEw4BiYhIBxQAIiIZSgEgIpKhFAAiIhlKASAikqEUACIiGUoBICKSof4/MZ/UEEw1FIsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(num_iterations), cost_list)\n",
    "plt.title(\"Visualization of Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_no_library = np.where(np.dot(X_test, weights) < 1, 0, 1)\n",
    "y_predict_no_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.17      0.26        35\n",
      "           1       0.42      0.81      0.55        26\n",
      "\n",
      "    accuracy                           0.44        61\n",
      "   macro avg       0.48      0.49      0.41        61\n",
      "weighted avg       0.49      0.44      0.39        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_predict_no_library))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Compare SVM results with Logistic Regression - (4 points)\n",
    "\n",
    "Which model performs better here? Compare your results with the logistic regression. You can use libraries for this task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.80      0.67        35\n",
      "           1       0.46      0.23      0.31        26\n",
      "\n",
      "    accuracy                           0.56        61\n",
      "   macro avg       0.52      0.52      0.49        61\n",
      "weighted avg       0.53      0.56      0.52        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "regression_model = LogisticRegression()\n",
    "regression_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_lr = model.predict(X_test)\n",
    "print(classification_report(y_test,y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be no difference in terms of the classification accuracy between the sci-kit learn logistic regression model and the sci-kit learn support vector machine model. Both models have congruent values for precision, recall, and accuracy. Notably, both of these models outperformed the no-library SVM implementation in accuracy  and precision, but not recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 - Apply a kernel function to improve SVM performance (4 points)\n",
    "\n",
    "Use the Scikit-learn library and apply a kernel function to improve the SVM performance. Check if this is possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_svm = svm.SVC(kernel = 'linear')\n",
    "kernel_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.69      0.74        35\n",
      "           1       0.65      0.77      0.70        26\n",
      "\n",
      "    accuracy                           0.72        61\n",
      "   macro avg       0.72      0.73      0.72        61\n",
      "weighted avg       0.73      0.72      0.72        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kernel_svm_predict = kernel_svm.predict(X_test)\n",
    "print(classification_report(y_test, kernel_svm_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of a linear kernel in the support vector machine increases the accuracy, precision, and recall metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
